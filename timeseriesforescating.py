# -*- coding: utf-8 -*-
"""timeseriesforescating.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/139UG4SjjsIb-ZuT6dnOk-A5SjYNXTEVb
"""

# =========================================
# PART 1: DOWNLOAD & LOAD UCI ELECTRICITY DATASET
# =========================================

!wget -O electricity.zip https://archive.ics.uci.edu/static/public/235/individual+household+electric+power+consumption.zip

!unzip -o electricity.zip

import pandas as pd
import numpy as np

# Load text file
df = pd.read_csv(
    "household_power_consumption.txt",
    sep=';',
    parse_dates={'Datetime': ['Date', 'Time']},
    infer_datetime_format=True,
    na_values='?',
    low_memory=False
)

# Keep only the main power column
df = df[['Datetime', 'Global_active_power']]
df = df.rename(columns={'Global_active_power': 'Load'})
df = df.dropna()

# Convert to float
df['Load'] = df['Load'].astype(float)
df = df.set_index("Datetime")

df.head()

# =========================================
# PART 2: CLEANING + HOURLY RESAMPLING
# =========================================

df = df.resample("H").mean()
df = df.resample("H").mean()

# FIX NaN after resampling
df = df.fillna(method="ffill")

# Train-test split (80/20)
split_index = int(len(df) * 0.8)
train_df = df.iloc[:split_index]
test_df = df.iloc[split_index:]



train_df.shape, test_df.shape

# =========================================
# PART 3: NORMALIZATION & SEQUENCE CREATION
# =========================================

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train_df)
test_scaled = scaler.transform(test_df)

def create_sequences(data, window):
    X, y = [], []
    for i in range(len(data)-window):
        X.append(data[i:i+window])
        y.append(data[i+window])
    return np.array(X), np.array(y)

window = 24  # 24-hour lookback

X_train, y_train = create_sequences(train_scaled, window)
X_test, y_test = create_sequences(test_scaled, window)

X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test  = X_test.reshape((X_test.shape[0],  X_test.shape[1], 1))

X_train.shape, X_test.shape

# =========================================
# PART 4: HYPEROPT + LSTM OBJECTIVE FUNCTION
# =========================================

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from hyperopt import hp, fmin, tpe, STATUS_OK, Trials
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf

space = {
    "units": hp.choice("units", [32, 64, 128]),
    "dropout": hp.uniform("dropout", 0.0, 0.4),
    "lr": hp.loguniform("lr", np.log(1e-4), np.log(5e-3)),
    "batch_size": hp.choice("batch_size", [16, 32, 64]),
    "epochs": hp.choice("epochs", [10, 15, 20])
}

def objective(params):

    model = Sequential([
        LSTM(params["units"], return_sequences=False, input_shape=(window, 1)),
        Dropout(params["dropout"]),
        Dense(1)
    ])

    optimizer = Adam(learning_rate=params["lr"])
    model.compile(loss="mse", optimizer=optimizer)

    model.fit(
        X_train, y_train,
        batch_size=params["batch_size"],
        epochs=params["epochs"],
        verbose=0
    )

    preds = model.predict(X_test, verbose=0)
    rmse = np.sqrt(mean_squared_error(y_test, preds))

    return {"loss": rmse, "status": STATUS_OK}

# =========================================
# PART 5: RUN HYPEROPT SEARCH
# =========================================

trials = Trials()

best_params = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=20,
    trials=trials
)

best_params

# =========================================
# PART 6: FINAL MODEL TRAINING
# =========================================

units_choices = [32, 64, 128]
batch_choices = [16, 32, 64]
epoch_choices = [10, 15, 20]

best = {
    "units": units_choices[best_params["units"]],
    "dropout": best_params["dropout"],
    "lr": best_params["lr"],
    "batch_size": batch_choices[best_params["batch_size"]],
    "epochs": epoch_choices[best_params["epochs"]]
}

print("BEST HYPERPARAMETERS:", best)

model = Sequential([
    LSTM(best["units"], input_shape=(window, 1)),
    Dropout(best["dropout"]),
    Dense(1)
])

model.compile(loss="mse", optimizer=Adam(learning_rate=best["lr"]))

history = model.fit(
    X_train, y_train,
    epochs=best["epochs"],
    batch_size=best["batch_size"],
    verbose=1
)

# =========================================
# PART 7: EVALUATION + PLOTTING
# =========================================

preds = model.predict(X_test)
preds_inv = scaler.inverse_transform(preds)
y_test_inv = scaler.inverse_transform(y_test)

rmse = np.sqrt(mean_squared_error(y_test_inv, preds_inv))
mae = mean_absolute_error(y_test_inv, preds_inv)

print("RMSE:", rmse)
print("MAE:", mae)

import matplotlib.pyplot as plt

plt.figure(figsize=(15,5))
plt.plot(y_test_inv[:500], label="Actual")
plt.plot(preds_inv[:500], label="Predicted")
plt.legend()
plt.title("Electricity Load Forecasting (LSTM + Hyperopt)")
plt.show()